{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression Classifier (From Scratch)\n",
        "### New dataset: IBM Telco Customer Churn\n",
        "\n",
        "This notebook is written in a *workshop style*: every code cell is preceded by a markdown cell that explains **what** and **why**.\n",
        "\n",
        "**Goal:** Train a Logistic Regression model to predict whether a customer will churn (leave) and explain results with charts.\n",
        "\n",
        "**You will learn:** data loading → cleaning → preprocessing → training → thresholding → evaluation → interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup (install packages)\n",
        "Run this only once in your environment (terminal or notebook). If you're already using your existing venv from your other labs, you can skip.\n",
        "\n",
        "The workshop repo you referenced also uses a requirements file approach; we do the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!python -m pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Import libraries\n",
        "We import standard data + ML libraries, and metrics for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Download the dataset (IBM Telco Customer Churn)\n",
        "We download a CSV directly from IBM's public GitHub (easy + reproducible). If you're offline, download manually and place it in `data/raw/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "DATA_DIR = Path(\"data/raw\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
        "csv_path = DATA_DIR / \"telco_churn.csv\"\n",
        "\n",
        "if not csv_path.exists():\n",
        "    df = pd.read_csv(url)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Quick data check\n",
        "We confirm columns, types, missing values, and how the target looks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "print(df.shape)\n",
        "df.info()\n",
        "print(df[\"Churn\"].value_counts(dropna=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Clean + define target\n",
        "In this dataset, `Churn` is 'Yes'/'No'. Logistic Regression needs numeric labels.\n",
        "\n",
        "- We map: Yes→1, No→0\n",
        "- We also clean `TotalCharges` because it often comes as text with blanks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "df = df.copy()\n",
        "\n",
        "# Target\n",
        "df[\"Churn\"] = df[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# TotalCharges cleanup (common issue: blank strings)\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "# Optional: drop customerID (it's an identifier, not a predictive feature)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "\n",
        "df.isna().sum().sort_values(ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Split into features (X) and target (y)\n",
        "This matches the workshop pattern:\n",
        "- X = inputs/features\n",
        "- y = label/target\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "target_col = \"Churn\"\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Train/Test Split\n",
        "We keep a test set to evaluate on unseen data.\n",
        "\n",
        "**Stratify** keeps the churn ratio similar in train/test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
        "print(\"Churn rate train:\", y_train.mean().round(3), \"test:\", y_test.mean().round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Build preprocessing (numeric + categorical)\n",
        "This is the main upgrade vs the simple workshop example (hours studied → pass/fail).\n",
        "\n",
        "- Numeric columns: impute median + scale\n",
        "- Categorical columns: impute most frequent + one-hot encode\n",
        "\n",
        "We keep everything inside a **Pipeline** so it’s reproducible and avoids data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
        "\n",
        "numeric_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipe, num_cols),\n",
        "        (\"cat\", categorical_pipe, cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "print(\"Numeric cols:\", len(num_cols), \"Categorical cols:\", len(cat_cols))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train Logistic Regression model\n",
        "Logistic Regression outputs **probabilities** using the sigmoid function.\n",
        "\n",
        "We set `max_iter` high to avoid convergence warnings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", model)\n",
        "])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Model trained ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Predict probabilities and labels\n",
        "The workshop explains classification like:\n",
        "- if p(X) >= 0.5 → Class 1\n",
        "- else → Class 0\n",
        "\n",
        "We'll start with 0.5, then we will tune it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "y_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "threshold = 0.5\n",
        "y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "y_proba[:10], y_pred[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Evaluate: Confusion matrix + classification report\n",
        "These metrics translate directly to real-world outcomes:\n",
        "- False positives: customers we think will churn but won't\n",
        "- False negatives: customers who will churn but we miss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\n",
        "\", cm)\n",
        "print(\"\n",
        "Classification Report:\n",
        "\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) ROC Curve + AUC\n",
        "ROC AUC measures how well the model ranks churners above non-churners across all thresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC AUC:\", round(auc, 4))\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Precision–Recall curve (often better for churn)\n",
        "If the positive class is rare, PR curves can be more informative.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "ap = average_precision_score(y_test, y_proba)\n",
        "print(\"Average Precision:\", round(ap, 4))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Choose a better threshold (business decision)\n",
        "Just like your predictive maintenance project used thresholds + persistence to control noise,\n",
        "classification uses a probability threshold to control false alarms vs missed churners.\n",
        "\n",
        "Here we pick the threshold that gives at least a target recall (e.g., 0.75) if you want to catch churners.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "target_recall = 0.75\n",
        "\n",
        "# Find smallest threshold that achieves recall >= target\n",
        "# (simple sweep over sorted probabilities)\n",
        "candidates = np.unique(np.round(y_proba, 4))\n",
        "best_thr = 0.5\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "for thr in candidates:\n",
        "    pred = (y_proba >= thr).astype(int)\n",
        "    r = recall_score(y_test, pred)\n",
        "    if r >= target_recall:\n",
        "        best_thr = thr\n",
        "        break\n",
        "\n",
        "print(\"Chosen threshold for recall >= \", target_recall, \"is:\", best_thr)\n",
        "\n",
        "y_pred_tuned = (y_proba >= best_thr).astype(int)\n",
        "print(\"\n",
        "Tuned confusion matrix:\n",
        "\", confusion_matrix(y_test, y_pred_tuned))\n",
        "print(\"\n",
        "Tuned report:\n",
        "\", classification_report(y_test, y_pred_tuned, digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Interpretability: which features push churn up/down?\n",
        "Logistic regression is interpretable: each feature has a coefficient.\n",
        "\n",
        "We extract feature names from the pipeline and show the largest positive/negative coefficients.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Get feature names after preprocessing\n",
        "ohe = clf.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
        "\n",
        "feature_names = np.concatenate([np.array(num_cols), cat_feature_names])\n",
        "\n",
        "coefs = clf.named_steps[\"model\"].coef_[0]\n",
        "coef_df = pd.DataFrame({\"feature\": feature_names, \"coef\": coefs}).sort_values(\"coef\")\n",
        "\n",
        "print(\"Top features decreasing churn probability:\")\n",
        "display(coef_df.head(12))\n",
        "\n",
        "print(\"Top features increasing churn probability:\")\n",
        "display(coef_df.tail(12))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) Save outputs (metrics + charts)\n",
        "This matches the MLOps discipline from your other project: keep artifacts for grading/auditing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "metrics = {\n",
        "    \"roc_auc\": float(auc),\n",
        "    \"average_precision\": float(ap),\n",
        "    \"default_threshold\": float(threshold),\n",
        "    \"tuned_threshold\": float(best_thr),\n",
        "}\n",
        "\n",
        "(Path(\"outputs\") / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
        "print(\"Saved outputs/metrics.json\")\n",
        "\n",
        "# Save charts\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.savefig(OUT_DIR / \"roc_curve.png\", dpi=160, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.savefig(OUT_DIR / \"pr_curve.png\", dpi=160, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved outputs/roc_curve.png and outputs/pr_curve.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) 3 Real‑World Talking Points (use these in your presentation)\n",
        "1. **Prediction → Action:** We turn a churn probability into a decision (who to contact, what offer to send). The threshold controls cost vs savings.\n",
        "2. **Charts → Trust:** Confusion matrix shows the *types* of mistakes; ROC/PR curves show trade‑offs. Stakeholders can choose a threshold aligned with business risk.\n",
        "3. **Process → Reliability:** A reproducible pipeline (split, preprocessing, model, saved metrics) prevents accidental leakage and makes results auditable and repeatable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17) Comparison to the workshop notebook\n",
        "The workshop uses a simple 1‑feature example (e.g., study hours → pass/fail) to teach the sigmoid + 0.5 cutoff concept.\n",
        "We keep the same logic but scale it to a real dataset:\n",
        "\n",
        "- **Workshop:** 1 feature, easy plot, focus on theory and the 0.5 rule.\n",
        "- **This notebook:** many features (numeric + categorical), full preprocessing pipeline, ROC/PR charts, threshold tuning, and model interpretation.\n",
        "\n",
        "Key link to workshop idea: classification rule `p(X) >= 0.5` (or tuned threshold) → class 1."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}